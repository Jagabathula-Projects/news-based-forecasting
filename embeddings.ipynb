{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafbeb09-dd90-4380-86de-d05572636e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunchak\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from gensim import models, corpora\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle as cp \n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from itertools import chain\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0a2ea",
   "metadata": {},
   "source": [
    "### Gensim preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56bacc6-cd8c-4968-866c-f83da185f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = sw.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa783ca5-b8f6-4371-a974-6f462d229935",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = '/data1/sunchak/news_dataset/toi/articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6a5e30-c5c1-4f36-9931-ba597a3426d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('toi.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c724b8-3ba2-45ce-8f6e-d25a20f25b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel.load('toi.LDA.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab3a6c-13f0-470b-a81b-68f46078a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = models.Word2Vec.load('toi.W2V.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac91a75-1527-416f-bcf1-7815f64563c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5dcbcbd",
   "metadata": {},
   "source": [
    "### Computing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e400c-e837-4405-bcca-36336b53fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_w2v_emb(day_docs):\n",
    "    day_w2v = []\n",
    "    for doc in day_docs:\n",
    "        for wrd in doc:\n",
    "            try:\n",
    "                day_w2v.append(w2v.wv[wrd])\n",
    "            except Exception as error:\n",
    "                print(\"word not found: \", wrd) \n",
    "    daily_w2v_emb = np.array(day_w2v).mean(axis=0)\n",
    "    return daily_w2v_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa069274-d12a-4974-b813-04a399dffa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_lda_embedding(day_docs):\n",
    "    '''\n",
    "    Computing the raw topic probabilites without any aggregation\n",
    "    '''\n",
    "    day_corpus = [dictionary.doc2bow(article) for article in day_docs]\n",
    "    day_topics = []\n",
    "    \n",
    "    for d in day_corpus:\n",
    "        try:\n",
    "            dtop = [k[1] for k in lda.get_document_topics(d,minimum_probability=0.0)]\n",
    "            \n",
    "            day_topics.append(dtop)\n",
    "        except Exception as error:\n",
    "            print(\"An exception occurred in the function:\", error) \n",
    "    return day_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a68c38-d757-4e5f-832e-9119afa61aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For training data\n",
    "\n",
    "emb_lda_tr = []\n",
    "emb_w2v_tr = []\n",
    "dates = []\n",
    "yr = '2006-2012'\n",
    "\n",
    "for k in sorted(os.listdir(os.path.join(corpus_path,str(yr)))):\n",
    "    print(k)\n",
    "    day_docs = [nltk.word_tokenize(json.loads(d)['text'].lower()) for d in open(os.path.join(corpus_path,str(yr),k)).readlines()]\n",
    "\n",
    "    #daily_topics = daily_lda_embedding(day_docs)\n",
    "    daily_w2v = daily_w2v_emb(day_docs)\n",
    "\n",
    "    #emb_lda_tr.append(daily_topics)\n",
    "    emb_w2v_tr.append(daily_w2v)\n",
    "\n",
    "## cp.dump(emb_lda_tr,open('raw_lda_train.list','wb'))\n",
    "## emb_lda_tr = cp.load(open('raw_lda_train.list','rb'))\n",
    "\n",
    "### max\n",
    "lda_train_max = []\n",
    "for i,daily_docs in enumerate(emb_lda_tr):\n",
    "    tmp = np.array(daily_docs)\n",
    "    lda_train_max.append(tmp.max(axis=0))\n",
    "\n",
    "\n",
    "### avg\n",
    "lda_train_avg = []\n",
    "for i,daily_docs in enumerate(emb_lda_tr):\n",
    "    tmp = np.array(daily_docs)        \n",
    "    lda_train_avg.append(tmp.mean(axis=0))\n",
    "                    \n",
    "    \n",
    "\n",
    "# np.save('toi.maxLDA_train.npy',np.array(lda_train_max))\n",
    "# np.save('toi.avgLDA_train.npy',np.array(lda_train_avg))\n",
    "\n",
    "# np.save('toi.w2v_train.npy',emb_w2v_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6565a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db2802-29f3-4c69-8917-31bda0510d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For testing data\n",
    "\n",
    "emb_lda_tst = []\n",
    "emb_w2v_tst = []\n",
    "dates = []\n",
    "test_years = range(2013,2021)\n",
    "for yr in test_years:\n",
    "    for k in sorted(os.listdir(os.path.join(corpus_path,str(yr)))):\n",
    "            day_docs = [nltk.word_tokenize(json.loads(d)['text'].lower()) for d in open(os.path.join(corpus_path,str(yr),k)).readlines()]\n",
    "            \n",
    "            daily_topics = daily_lda_embedding(day_docs)\n",
    "            daily_w2v = daily_w2v_emb(day_docs)\n",
    "            \n",
    "            emb_lda_tst.append(daily_topics)\n",
    "            emb_w2v_tst.append(daily_w2v)\n",
    "\n",
    "## cp.dump(emb_lda_tst,open('raw_lda_test.list','wb'))\n",
    "## emb_lda_tst = cp.load(open('raw_lda_test.list','rb'))\n",
    "\n",
    "### max\n",
    "lda_test_max = []\n",
    "for i,daily_docs in enumerate(emb_lda_tst):\n",
    "    tmp = np.array(daily_docs)\n",
    "    lda_train.append(tmp.max(axis=0))\n",
    "\n",
    "\n",
    "### avg\n",
    "lda_test_avg = []\n",
    "for i,daily_docs in enumerate(raw_lda):#[:100]:\n",
    "    tmp = np.array(daily_docs)        \n",
    "    lda_train.append(tmp.mean(axis=0))\n",
    "                    \n",
    "    \n",
    "\n",
    "np.save('toi.maxLDA_train.npy',np.array(lda_train_max))\n",
    "np.save('toi.avgLDA_train.npy',np.array(lda_train_avg))\n",
    "\n",
    "np.save('toi.w2v_train.npy',emb_w2v_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd964d-d31a-4776-9edc-7f15969d48a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb42aa6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d1514",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c98ea829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for k in sorted(os.listdir(os.path.join(corpus_path,'2006-2012'))):\n",
    "    all_docs += [json.loads(d)['text'] for d in open(os.path.join(corpus_path,'2006-2012',k)).readlines()]\n",
    "\n",
    "texts = [[word for word in nltk.word_tokenize(document.lower()) if word not in stoplist] for document in all_docs]\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=250)\n",
    "\n",
    "lda.save('toi.LDA.lda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac84307",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f7faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "all_docs = []\n",
    "for k in sorted(os.listdir(os.path.join(corpus_path,'2006-2012'))):\n",
    "    all_docs += list(chain.from_iterable([nltk.sent_tokenize(json.loads(d)['text']) for d in open(os.path.join(corpus_path,'2006-2012',k)).readlines()]))\n",
    "\n",
    "i=0\n",
    "for doc in all_docs:\n",
    "    sentences.append(utils.simple_preprocess(doc))\n",
    "\n",
    "w2vmodel = models.Word2Vec(sentences=sentences, vector_size=250, window=5, min_count=10, workers=10)\n",
    "   \n",
    "w2vmodel.save('toi.W2V.w2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4706c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hyderabad', 0.768227219581604),\n",
       " ('kolkata', 0.7237099409103394),\n",
       " ('pune', 0.6968643069267273),\n",
       " ('ahmedabad', 0.6388577818870544),\n",
       " ('chennai', 0.613448977470398)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar(positive=['mumbai', 'bangalore'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d273af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
