{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbeb09-dd90-4380-86de-d05572636e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle as cp \n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from itertools import chain\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d36c9",
   "metadata": {},
   "source": [
    "### Gensim preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bacc6-cd8c-4968-866c-f83da185f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = sw.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa783ca5-b8f6-4371-a974-6f462d229935",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = '/data1/sunchak/news_dataset/toi/articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a5e30-c5c1-4f36-9931-ba597a3426d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('toi.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c724b8-3ba2-45ce-8f6e-d25a20f25b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel.load('toi.LDA.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab3a6c-13f0-470b-a81b-68f46078a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = models.Word2Vec.load('toi.W2V.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac91a75-1527-416f-bcf1-7815f64563c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbb82e2",
   "metadata": {},
   "source": [
    "### Computing the embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e400c-e837-4405-bcca-36336b53fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_w2v_emb(day_docs):\n",
    "    day_w2v = []\n",
    "    for doc in day_docs:\n",
    "        for wrd in doc:\n",
    "            try:\n",
    "                day_w2v.append(w2v.wv[wrd])\n",
    "            except Exception as error:\n",
    "                print(\"word not found: \", wrd) \n",
    "    daily_w2v_emb = np.array(day_w2v).mean(axis=0)\n",
    "    return daily_w2v_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa069274-d12a-4974-b813-04a399dffa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_lda_embedding(day_docs):\n",
    "    '''\n",
    "    Computing the raw topic probabilites without any aggregation\n",
    "    '''\n",
    "    day_corpus = [dictionary.doc2bow(article) for article in day_docs]\n",
    "    day_topics = []\n",
    "    \n",
    "    for d in day_corpus:\n",
    "        try:\n",
    "            dtop = [k[1] for k in lda.get_document_topics(d,minimum_probability=0.0)]\n",
    "            \n",
    "            day_topics.append(dtop)\n",
    "        except Exception as error:\n",
    "            print(\"An exception occurred in the function:\", error) \n",
    "    return day_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a68c38-d757-4e5f-832e-9119afa61aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For training data\n",
    "\n",
    "emb_lda_tr = []\n",
    "emb_w2v_tr = []\n",
    "dates = []\n",
    "yr = '2006-2012'\n",
    "\n",
    "for k in sorted(os.listdir(os.path.join(corpus_path,str(yr)))):\n",
    "    print(k)\n",
    "    day_docs = [nltk.word_tokenize(json.loads(d)['text'].lower()) for d in open(os.path.join(corpus_path,str(yr),k)).readlines()]\n",
    "\n",
    "    #daily_topics = daily_lda_embedding(day_docs)\n",
    "    daily_w2v = daily_w2v_emb(day_docs)\n",
    "\n",
    "    #emb_lda_tr.append(daily_topics)\n",
    "    emb_w2v_tr.append(daily_w2v)\n",
    "\n",
    "## cp.dump(emb_lda_tr,open('raw_lda_train.list','wb'))\n",
    "## emb_lda_tr = cp.load(open('raw_lda_train.list','rb'))\n",
    "\n",
    "### max\n",
    "lda_train_max = []\n",
    "for i,daily_docs in enumerate(emb_lda_tr):\n",
    "    tmp = np.array(daily_docs)\n",
    "    lda_train_max.append(tmp.max(axis=0))\n",
    "\n",
    "\n",
    "### avg\n",
    "lda_train_avg = []\n",
    "for i,daily_docs in enumerate(emb_lda_tr):\n",
    "    tmp = np.array(daily_docs)        \n",
    "    lda_train_avg.append(tmp.mean(axis=0))\n",
    "                    \n",
    "    \n",
    "\n",
    "# np.save('toi.maxLDA_train.npy',np.array(lda_train_max))\n",
    "# np.save('toi.avgLDA_train.npy',np.array(lda_train_avg))\n",
    "\n",
    "# np.save('toi.w2v_train.npy',emb_w2v_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db2802-29f3-4c69-8917-31bda0510d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For training data\n",
    "\n",
    "emb_lda_tst = []\n",
    "emb_w2v_tst = []\n",
    "dates = []\n",
    "test_years = range(2013,2021)\n",
    "for yr in test_years:\n",
    "    for k in sorted(os.listdir(os.path.join(corpus_path,str(yr)))):\n",
    "            day_docs = [nltk.word_tokenize(json.loads(d)['text'].lower()) for d in open(os.path.join(corpus_path,str(yr),k)).readlines()]\n",
    "            \n",
    "            daily_topics = daily_lda_embedding(day_docs)\n",
    "            daily_w2v = daily_w2v_emb(day_docs)\n",
    "            \n",
    "            emb_lda_tst.append(daily_topics)\n",
    "            emb_w2v_tst.append(daily_w2v)\n",
    "\n",
    "## cp.dump(emb_lda_tst,open('raw_lda_test.list','wb'))\n",
    "## emb_lda_tst = cp.load(open('raw_lda_test.list','rb'))\n",
    "\n",
    "### max\n",
    "lda_test_max = []\n",
    "for i,daily_docs in enumerate(emb_lda_tst):\n",
    "    tmp = np.array(daily_docs)\n",
    "    lda_train.append(tmp.max(axis=0))\n",
    "\n",
    "\n",
    "### avg\n",
    "lda_test_avg = []\n",
    "for i,daily_docs in enumerate(raw_lda):#[:100]:\n",
    "    tmp = np.array(daily_docs)        \n",
    "    lda_train.append(tmp.mean(axis=0))\n",
    "                    \n",
    "    \n",
    "\n",
    "np.save('toi.maxLDA_train.npy',np.array(lda_train_max))\n",
    "np.save('toi.avgLDA_train.npy',np.array(lda_train_avg))\n",
    "\n",
    "np.save('toi.w2v_train.npy',emb_w2v_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd964d-d31a-4776-9edc-7f15969d48a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1773a27-ef04-4494-8853-9f36c4999cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
