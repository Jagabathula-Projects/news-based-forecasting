{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cer7KsrCcd4a","outputId":"3edf27e6-5c8a-45f4-e6f6-cbf1596efa24","executionInfo":{"status":"ok","timestamp":1698723825261,"user_tz":300,"elapsed":19069,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VQkvHmEpdWou","executionInfo":{"status":"ok","timestamp":1698724211111,"user_tz":300,"elapsed":5740,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}}},"outputs":[],"source":["import sys\n","import os\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","import json\n","import numpy\n","import matplotlib.pyplot as plt\n","from pandas import read_csv\n","import math\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","import pickle as cp\n","from tensorflow import keras\n","import tensorflow as tf\n","from keras import backend as K\n","import datetime as dt\n","from tensorflow.keras.layers.experimental.preprocessing import Normalization\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"FysU1WbCdZQi","executionInfo":{"status":"ok","timestamp":1698724222071,"user_tz":300,"elapsed":4324,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}}},"outputs":[],"source":["embeddings_path = '/content/drive/MyDrive/news-based-forecasting/embeddings/'\n","test_event_embeddings = np.load(f'{embeddings_path}events_test.npy')\n","train_event_embeddings = np.load(f'{embeddings_path}events_train.npy')\n","train_topic_embeddings = np.load(f'{embeddings_path}toi.maxLDA_train.npy')\n","test_topic_embeddings = np.load(f'{embeddings_path}toi.maxLDA_test.npy')\n","train_word_embeddings = np.load(f'{embeddings_path}toi.W2V_train.npy')\n","test_word_embeddings = np.load(f'{embeddings_path}toi.W2V_test.npy')\n","train_doc_embeddings = np.load(f'{embeddings_path}toi.D2V_train.npy')\n","test_doc_embeddings = np.load(f'{embeddings_path}toi.D2V_test.npy')\n","# concatenate train and test\n","all_event_embeddings = np.vstack((train_event_embeddings, test_event_embeddings))\n","all_word_embeddings = np.vstack((train_word_embeddings, test_word_embeddings))\n","all_topic_embeddings = np.vstack((train_topic_embeddings, test_topic_embeddings))\n","all_doc_embeddings = np.vstack((train_doc_embeddings, test_doc_embeddings))\n","all_embeddings_dict = {'LDA': all_topic_embeddings, 'w2v': all_word_embeddings, 'events': all_event_embeddings, 'd2v': all_doc_embeddings}"]},{"cell_type":"code","source":["def addSpanLag(data, events=[], look_back=7, look_ahead = 7, season=0,span=1):\n","    ### Adding span (required for weekly, monthly average data)\n","    mnth_days = [31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,29,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,29,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31]\n","    events = events[:3652] ### upto dec 31, 2015\n","    ts = []\n","    if span>1:\n","        nts = []\n","        ind = 0\n","        i=0\n","        while ind < len(events):\n","            span = mnth_days[i]\n","            tmpts = events[ind:ind+span]\n","            vals = [k for k in tmpts]\n","            pr = sum(vals)/len(vals)\n","            nts.append(pr)\n","            ind+=span\n","            i+=1\n","        ts = nts\n","    # if not convert_to_Xy:\n","    #     return ts\n","    events = np.array(ts)\n","    print(events.shape)\n","\n","    ### Adding lag (based on look back and look ahead)\n","\n","    if len(events) > 0:\n","        input = np.append(data,events,axis=1)\n","    else:\n","        input = data\n","    if season>0:\n","        season = np.array([k%season for k in range(data.shape[0])])\n","        season = season.reshape(len(season),1)\n","        input = np.append(input,season,axis=1)\n","\n","\n","    ind = 0\n","    X_data = []\n","    y_data = []\n","\n","    while ind + look_back + look_ahead < len(data):\n","\n","        X = input[ind:ind+look_back]\n","        Y = data[ind+look_back+look_ahead]\n","        X_data.append(X)\n","        y_data.append(Y)\n","        ind+=1\n","    X_data = np.array(X_data)\n","    y_data = np.array(y_data)\n","\n","    return X_data, y_data"],"metadata":{"id":"SlvfA94o-toD","executionInfo":{"status":"ok","timestamp":1698724223787,"user_tz":300,"elapsed":240,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QZ2iWMzSd_gq","executionInfo":{"status":"ok","timestamp":1698684185110,"user_tz":300,"elapsed":90,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}}},"outputs":[],"source":["def ren_train(X, y, hidden_units=100, learning_rate=0.001, epochs=60, batch_size=256,activation='relu',\\\n","              dropout=0.0,recurrent_dropout=0.0,regularizer='l2',optimizer='adam',kernel_regularizer='l2'):\n","    model = Sequential()\n","    model.add(LSTM(hidden_units, activation=activation,  kernel_regularizer='l2',#recurrent_regularizer='l2',\n","                   dropout=dropout, recurrent_dropout=recurrent_dropout,unroll=True,\n","                   input_shape=(X.shape[1], X.shape[-1])))\n","    model.add(Dense(1))\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","    model.compile(optimizer=optimizer, loss='mse')#,metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n","    history = model.fit(X, y, epochs=epochs, verbose=1)\n","    plt.plot(history.history['loss'])\n","    plt.show()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvoUnEbQCSFh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUzyRHYIBGvA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fKO9An6U4N8"},"outputs":[],"source":["diseases = ['malaria','dengue','ili']\n","\n","train_split = 84  # - 1 #if using price diff\n","season = 0\n","normalization = True\n","minmaxscale = False\n","\n","path='/content/gdrive/MyDrive/Colab Notebooks/REN/'\n","fileformat = path+'health/Monthly %s cases in india between 2006 and 2015.csv'\n","\n","output_folder = 'health'\n","\n","### All together\n","dropouts = [0.0, 0.1, 0.2, 0.3]\n","activation = 'tanh'#,'relu']\n","epochs=[100]\n","optimizers = ['adam']#,'rmsprop']\n","units = [1,2, 5,10]\n","kernel_regularizers = ['l2']\n","#learning_rates = [0.001,0.0001]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTRSJNJP8ah5"},"outputs":[],"source":["i=0\n","for disease in diseases:\n","  for kernel_regularizer in kernel_regularizers:\n","    for optimizer in optimizers:\n","      for drpt in dropouts:\n","        for hidden_units in units:\n","              epochs=70\n","              learning_rate=0.001\n","              model_suffix = \"disease=%s_season=%d_activation=%s_optimizer=%s_learning_rate=%.4f_dropout=%.2f_units=%s_epochs=%d_train=2010-15_kreg=%s\"\\\n","              %(disease,season,activation,optimizer,learning_rate,drpt,hidden_units,epochs,kernel_regularizer)\n","              print(str(i),model_suffix)\n","              i+=1\n","\n","              incdnc = pd.read_csv(fileformat%(disease),thousands=r',')['Cases'].astype(float)\n","\n","              ### price diff\n","              #food_price = food_price.diff().dropna()\n","              ### Return rate\n","              #food_price = np.log(food_price).diff()[1:]\n","              incdnc = np.array(incdnc).reshape(incdnc.shape[0],1)\n","\n","              mean = np.mean(incdnc)\n","              std = np.std(incdnc)\n","              maxx = incdnc.max()\n","              minn = incdnc.min()\n","              print (incdnc[0])\n","              if normalization:\n","                  incdnc = (incdnc - mean)/std\n","              elif minmaxscale:\n","\n","                  incdnc = (incdnc - min)/(maxx - minn)\n","\n","              ### Events\n","              events_train = np.load(os.path.join(path,'embeddings/events_train.npy'))\n","              events_test = np.load(os.path.join(path,'embeddings/events_test.npy'))\n","              events = np.append(events_train,events_test, axis=0)#[1:]\n","\n","              ### Topics\n","              topics_train = np.load(os.path.join(path,'embeddings/toi.maxLDA_train.npy'))\n","              topic_test = np.load(os.path.join(path,'embeddings/toi.maxLDA_test.npy'))\n","              topics = np.append(topics_train,topic_test, axis=0)#[1:]\n","\n","              ### Word2Vec\n","              w2v_train = np.load(os.path.join(path,'embeddings/toi.W2V_train.npy'))\n","              w2v_test = np.load(os.path.join(path,'embeddings/toi.W2V_test.npy'))\n","              w2v = np.append(w2v_train,w2v_test, axis=0)#[1:]\n","\n","              ### Doc2Vec\n","              d2v_train = np.load(os.path.join(path,'embeddings/toi.D2V_train.npy'))\n","              d2v_test = np.load(os.path.join(path,'embeddings/toi.D2V_test.npy'))\n","              d2v = np.append(d2v_train,d2v_test, axis=0)#[1:]\n","\n","              X_event, y_event   = addSpanLag(incdnc,events,look_back=3,look_ahead=0,season=season,span=30)\n","              X_event_train = X_event[:train_split]\n","              y_event_train = y_event[:train_split]\n","              X_event_test = X_event[train_split:]\n","              y_event_test = y_event[train_split:]\n","\n","              X_topics, y_topics   = addSpanLag(incdnc,topics,look_back=3,look_ahead=0,season=season,span=30)\n","              X_topics_train = X_topics[:train_split]\n","              y_topics_train = y_topics[:train_split]\n","              X_topics_test = X_topics[train_split:]\n","              y_topics_test = y_topics[train_split:]\n","\n","              X_w2v, y_w2v   = addSpanLag(incdnc,w2v,look_back=3,look_ahead=0,season=season,span=30)\n","              X_w2v_train = X_w2v[:train_split]\n","              y_w2v_train = y_w2v[:train_split]\n","              X_w2v_test = X_w2v[train_split:]\n","              y_w2v_test = y_w2v[train_split:]\n","\n","              X_d2v, y_d2v   = addSpanLag(incdnc,d2v,look_back=3,look_ahead=0,season=season,span=30)\n","              X_d2v_train = X_d2v[:train_split]\n","              y_d2v_train = y_d2v[:train_split]\n","              X_d2v_test = X_d2v[train_split:]\n","              y_d2v_test = y_d2v[train_split:]\n","\n","\n","              X_noevent, y_noevent   = addSpanLag(incdnc,look_back=3,look_ahead=0,season=season,span=30)\n","              X_noevent_train = X_noevent[:train_split]\n","              y_noevent_train = y_noevent[:train_split]\n","              X_noevent_test = X_noevent[train_split:]\n","              y_noevent_test = y_noevent[train_split:]\n","\n","\n","\n","              ### REN\n","              model_event = ren_train(X_event_train,y_event_train,hidden_units=hidden_units, \\\n","                                      activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs, kernel_regularizer=kernel_regularizer)\n","              model_event.save(os.path.join(path,output_folder+'_models','ren_'+model_suffix))\n","              print(model_event.summary())\n","              ### LSTM\n","              model_noevent = ren_train(X_noevent_train,y_noevent_train,hidden_units=hidden_units, \\\n","                                        activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n","              model_noevent.save(os.path.join(path,output_folder+'_models','lstm_'+model_suffix))\n","\n","\n","\n","\n","              others=True\n","\n","              if others:\n","                  ### LDA\n","                  model_topics = ren_train(X_topics_train,y_topics_train,hidden_units=hidden_units, \\\n","                                         activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n","                  model_topics.save(os.path.join(path,output_folder+'_models','lda_'+model_suffix))\n","                  ### W2V\n","                  model_w2v = ren_train(X_w2v_train,y_w2v_train,hidden_units=hidden_units, \\\n","                                         activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n","                  model_w2v.save(os.path.join(path,output_folder+'_models','w2v_'+model_suffix))\n","\n","                  ### D2V\n","                  model_d2v = ren_train(X_d2v_train,y_d2v_train,hidden_units=hidden_units, \\\n","                                         activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n","                  model_d2v.save(os.path.join(path,output_folder+'_models','d2v_'+model_suffix))\n","\n","              e_res_tr = model_event.evaluate(X_event_train,y_event_train)\n","              e_rmse_tr = np.sqrt(e_res_tr)*std\n","              e_res = model_event.evaluate(X_event_test,y_event_test)\n","              e_rmse = np.sqrt(e_res)*std\n","\n","\n","              l_res_tr = model_noevent.evaluate(X_noevent_train,y_noevent_train)\n","              l_rmse_tr = np.sqrt(l_res_tr)*std\n","              l_res = model_noevent.evaluate(X_noevent_test,y_noevent_test)\n","              l_rmse = np.sqrt(l_res)*std\n","\n","\n","              if others:\n","                t_res_tr = model_topics.evaluate(X_topics_train,y_topics_train)\n","                t_rmse_tr = np.sqrt(t_res_tr)*std\n","                t_res = model_topics.evaluate(X_topics_test,y_topics_test)\n","                t_rmse = np.sqrt(t_res)*std\n","\n","                w_res_tr = model_w2v.evaluate(X_w2v_train,y_w2v_train)\n","                w_rmse_tr = np.sqrt(w_res_tr)*std\n","                w_res = model_w2v.evaluate(X_w2v_test,y_w2v_test)\n","                w_rmse = np.sqrt(w_res)*std\n","\n","                d_res_tr = model_d2v.evaluate(X_d2v_train,y_d2v_train)\n","                d_rmse_tr = np.sqrt(d_res_tr)*std\n","                d_res = model_d2v.evaluate(X_d2v_test,y_d2v_test)\n","                d_rmse = np.sqrt(d_res)*std\n","\n","              print(\"%s,%d,%d,%s,%s,%s,%.4f,%.2f,%d,\\\n","              ren,%.3f,%.3f,\\\n","              lstm,%.3f,%.3f,\\\n","              lda,%.3f,%.3f,\\\n","              w2v,%.3f,%.3f,\\\n","              d2v,%.3f,%.3f,%s\\n\"\\\n","              %(disease,season,epochs,activation,optimizer,kernel_regularizer,learning_rate,drpt,hidden_units,\n","                 e_rmse_tr,e_rmse,\\\n","                 l_rmse_tr,l_rmse,\\\n","                t_rmse_tr,t_rmse,\\\n","                w_rmse_tr,w_rmse,\\\n","                d_rmse_tr,d_rmse,\\\n","                kernel_regularizer))\n","\n","\n"]},{"cell_type":"markdown","source":["### Compute prediction metrics"],"metadata":{"id":"YAnzh7lAkru5"}},{"cell_type":"code","source":["models_path = '/content/drive/MyDrive/news-based-forecasting/disease/models/'\n","data_path = '/content/drive/MyDrive/news-based-forecasting/disease/data/'\n","disease = 'malaria'\n","price_lag = 3\n","data_freq = 30\n","num_train_days = 84\n","embedding_type = 'events'\n","fileformat = data_path+'Monthly %s cases in india between 2006 and 2015.csv'\n","disease_data = pd.read_csv(fileformat%(disease),thousands=r',')['Cases'].astype(float)\n","disease_data = disease_data.values.reshape(disease_data.shape[0],1)\n","X_event, y_event = addSpanLag(disease_data, all_embeddings_dict['events'], look_back=price_lag, look_ahead=0, span=data_freq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjEqiXlsj9-n","executionInfo":{"status":"ok","timestamp":1698693198292,"user_tz":300,"elapsed":174,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}},"outputId":"64e50a50-79c9-4f36-d1ac-7d461104534e"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["(120, 250)\n"]}]},{"cell_type":"code","source":["np.ones(3).T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XypEB-nyodJW","executionInfo":{"status":"ok","timestamp":1698693086147,"user_tz":300,"elapsed":107,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}},"outputId":"e04d8c53-40ea-4837-95c7-e8d6011c0766"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["sum([np.ones(4), np.ones(4)])/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koSzLjAol59j","executionInfo":{"status":"ok","timestamp":1698686298385,"user_tz":300,"elapsed":177,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}},"outputId":"bcd2fb62-66c8-404f-ef81-468b4da65484"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2., 2., 2., 2.])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"cV1z0DO7pTMv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698725276107,"user_tz":300,"elapsed":8398,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}},"outputId":"e9218e3d-388d-4846-ad73-5e90d7409eea"},"outputs":[{"output_type":"stream","name":"stdout","text":["(120, 250)\n","(0,)\n","2/2 [==============================] - 0s 9ms/step\n","lstm test RMSE for dengue = 4424.064021289847\n","2/2 [==============================] - 0s 13ms/step\n","lstm test MAPE for dengue = 0.033278050711182096\n","naive test RMSE for dengue = 1052.6590285097213\n","2/2 [==============================] - 0s 12ms/step\n","ren test RMSE for dengue = 4253.610001367231\n","2/2 [==============================] - 0s 16ms/step\n","ren test MAPE for dengue = 0.032349256933722766\n","(120, 250)\n","2/2 [==============================] - 0s 16ms/step\n","LDA test RMSE for dengue = 5361.707877954422\n","2/2 [==============================] - 0s 12ms/step\n","LDA test MAPE for dengue = 0.041524526520138334\n","(120, 250)\n","2/2 [==============================] - 0s 7ms/step\n","w2v test RMSE for dengue = 9032.318051751188\n","2/2 [==============================] - 0s 11ms/step\n","w2v test MAPE for dengue = 0.073310707989533\n","(120, 250)\n","2/2 [==============================] - 0s 8ms/step\n","d2v test RMSE for dengue = 14137.932934436245\n","2/2 [==============================] - 0s 5ms/step\n","d2v test MAPE for dengue = 0.11673123867213304\n"]}],"source":["models_path = '/content/drive/MyDrive/news-based-forecasting/disease/models/'\n","data_path = '/content/drive/MyDrive/news-based-forecasting/disease/data/'\n","disease = 'dengue'\n","price_lag = 3\n","data_freq = 30\n","num_train_days = 84\n","embedding_type = 'events'\n","fileformat = data_path+'Monthly %s cases in india between 2006 and 2015.csv'\n","disease_data = pd.read_csv(fileformat%(disease),thousands=r',')['Cases'].astype(float)\n","disease_data = disease_data.values.reshape(disease_data.shape[0],1)\n","scaler = StandardScaler()\n","disease_data_scaled = scaler.fit_transform(disease_data)\n","mean, scale = scaler.mean_[0], scaler.scale_[0]\n","X_event, y_event = addSpanLag(disease_data_scaled, all_embeddings_dict[embedding_type], look_back=price_lag, look_ahead=0, span=data_freq)\n","X_lstm, y_lstm = addSpanLag(disease_data_scaled, look_back=price_lag, look_ahead=0, span=data_freq)\n","num_test_preds = y_lstm[num_train_days:].shape[0]\n","model_name = f'lstm_disease={disease}'\n","model = keras.models.load_model(os.path.join(models_path,model_name))\n","#print(f\"lstm train RMSE for {disease} = {scale*mean_squared_error(y_lstm[:num_train_days-1], model.predict(X_lstm[:num_train_days-1]), squared=False)}\")\n","print(f\"lstm test RMSE for {disease} = {scale*mean_squared_error(y_lstm[num_train_days:], model.predict(X_lstm[num_train_days:]), squared=False)}\")\n","  #model_name = f'{model_type}_crop={crop}_norm=True_season=0'\n","print(f\"lstm test MAPE for {disease} = {mean_absolute_percentage_error(scale*y_lstm[num_train_days:] + mean, scale*model.predict(X_lstm[num_train_days:]) + mean)}\")\n","print(f\"naive test RMSE for {disease} = {scale*mean_squared_error(y_lstm[num_train_days:], y_lstm[num_train_days-1:-1], squared=False)}\")\n","\n","\n","model_name = f'ren_disease={disease}'\n","model = keras.models.load_model(os.path.join(models_path,model_name))\n","print(f\"ren test RMSE for {disease} = {scale*mean_squared_error(y_event[num_train_days:], model.predict(X_event[num_train_days:]), squared=False)}\")\n","print(f\"ren test MAPE for {disease} = {mean_absolute_percentage_error(scale*y_event[num_train_days:] + mean, scale*model.predict(X_event[num_train_days:]) + mean)}\")\n","#print(f\"{embedding_type} train RMSE for {crop} = {mean_squared_error(y_event[:num_train_days-1], model.predict(X_event[:num_train_days-1]), squared=False)}\")\n","\n","embedding_type = 'LDA'\n","X_event, y_event = addSpanLag(disease_data_scaled, all_embeddings_dict[embedding_type], look_back=price_lag, look_ahead=0, span=data_freq)\n","model_name = f'lda_disease={disease}'\n","model = keras.models.load_model(os.path.join(models_path,model_name))\n","print(f\"LDA test RMSE for {disease} = {scale*mean_squared_error(y_event[num_train_days:], model.predict(X_event[num_train_days:]), squared=False)}\")\n","print(f\"LDA test MAPE for {disease} = {mean_absolute_percentage_error(scale*y_event[num_train_days:] + mean, scale*model.predict(X_event[num_train_days:]) + mean)}\")\n","#print(f\"{embedding_type} test RMSE for {crop} = {mean_squared_error(y_event[num_train_days-1:], model.predict(X_event[num_train_days-1:]), squared=False)}\")\n","\n","embedding_type = 'w2v'\n","X_event, y_event = addSpanLag(disease_data_scaled, all_embeddings_dict[embedding_type], look_back=price_lag, look_ahead=0, span=data_freq)\n","model_name = f'w2v_disease={disease}'\n","model = keras.models.load_model(os.path.join(models_path,model_name))\n","print(f\"w2v test RMSE for {disease} = {scale*mean_squared_error(y_event[num_train_days:], model.predict(X_event[num_train_days:]), squared=False)}\")\n","print(f\"w2v test MAPE for {disease} = {mean_absolute_percentage_error(scale*y_event[num_train_days:] + mean, scale*model.predict(X_event[num_train_days:]) + mean)}\")\n","#print(f\"{embedding_type} test RMSE for {crop} = {mean_squared_error(y_event[num_train_days-1:], model.predict(X_event[num_train_days-1:]), squared=False)}\")\n","\n","embedding_type = 'd2v'\n","X_event, y_event = addSpanLag(disease_data_scaled, all_embeddings_dict[embedding_type], look_back=price_lag, look_ahead=0, span=data_freq)\n","model_name = f'd2v_disease={disease}'\n","model = keras.models.load_model(os.path.join(models_path,model_name))\n","print(f\"d2v test RMSE for {disease} = {scale*mean_squared_error(y_event[num_train_days:], model.predict(X_event[num_train_days:]), squared=False)}\")\n","print(f\"d2v test MAPE for {disease} = {mean_absolute_percentage_error(scale*y_event[num_train_days:] + mean, scale*model.predict(X_event[num_train_days:]) + mean)}\")\n","#print(f\"{embedding_type} test RMSE for {crop} = {mean_squared_error(y_event[num_train_days-1:], model.predict(X_event[num_train_days-1:]), squared=False)}\")\n","# print(f\"ren test RMSE for {crop} = {np.sqrt(model.evaluate(X_event[num_train_days-1:], y_event[num_train_days-1:]))}\")\n","# print(f\"{model_type} test RMSE for {crop} = {diff_prices.std()*np.sqrt(model.evaluate(X_event[num_train_days:], y_event[num_train_days:]))}\")"]},{"cell_type":"code","source":["a = []\n","a = a[:100]"],"metadata":{"id":"PlXh2a4oBGii","executionInfo":{"status":"ok","timestamp":1698693445108,"user_tz":300,"elapsed":90,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["a[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"7KbaqSgkBPUH","executionInfo":{"status":"error","timestamp":1698693456267,"user_tz":300,"elapsed":118,"user":{"displayName":"Ashwin Venkataraman","userId":"00537900344781957242"}},"outputId":"257b5b6c-3ecf-4029-d242-c10a7fca543e"},"execution_count":39,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-6a1284577a36>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yCqOqLgRBPqa"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}