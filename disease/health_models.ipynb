{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cer7KsrCcd4a",
    "outputId": "27c0e99c-04b5-46c2-ba33-ddcddea8b424"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQkvHmEpdWou"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle as cp\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import datetime as dt\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FysU1WbCdZQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLPBV47U9HIF"
   },
   "outputs": [],
   "source": [
    "def addSpanLag(data, events=[], look_back=7, look_ahead = 7, season=0,span=1):\n",
    "    ### Adding span (required for weekly, monthly average data)\n",
    "    mnth_days = [31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,29,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,29,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "    events = events[:3652] ### upto dec 31, 2015\n",
    "    ts = []\n",
    "    if span>1:\n",
    "        nts = []\n",
    "        ind = 0\n",
    "        i=0\n",
    "        while ind < len(events):\n",
    "            span = mnth_days[i]\n",
    "            \n",
    "            tmpts = events[ind:ind+span]\n",
    "            vals = [k for k in tmpts]\n",
    "            pr = sum(vals)/len(vals)\n",
    "            nts.append(pr)\n",
    "            ind+=span\n",
    "            i+=1\n",
    "        ts = nts\n",
    "    # if not convert_to_Xy:\n",
    "    #     return ts\n",
    "    events = np.array(ts)\n",
    "\n",
    "\n",
    "    ### Adding lag (based on look back and look ahead)\n",
    "\n",
    "    if len(events) > 0:\n",
    "        input = np.append(data,events,axis=1)\n",
    "    else:\n",
    "        input = data\n",
    "    if season>0:\n",
    "        season = np.array([k%season for k in range(data.shape[0])])\n",
    "        season = season.reshape(len(season),1)\n",
    "        input = np.append(input,season,axis=1)\n",
    "\n",
    "\n",
    "    ind = 0\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    while ind + look_back + look_ahead < len(data):\n",
    "\n",
    "        X = input[ind:ind+look_back]\n",
    "        Y = data[ind+look_back+look_ahead]\n",
    "        X_data.append(X)\n",
    "        y_data.append(Y)\n",
    "        ind+=1\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ2iWMzSd_gq"
   },
   "outputs": [],
   "source": [
    "def ren_train(X, y, hidden_units=100, learning_rate=0.001, epochs=60, batch_size=256,activation='relu',\\\n",
    "              dropout=0.0,recurrent_dropout=0.0,regularizer='l2',optimizer='adam',kernel_regularizer='l2'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_units, activation=activation,  kernel_regularizer='l2',#recurrent_regularizer='l2',\n",
    "                   dropout=dropout, recurrent_dropout=recurrent_dropout,unroll=True,\n",
    "                   input_shape=(X.shape[1], X.shape[-1])))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mse')#,metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    history = model.fit(X, y, epochs=epochs, verbose=1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvoUnEbQCSFh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUzyRHYIBGvA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fKO9An6U4N8"
   },
   "outputs": [],
   "source": [
    "diseases = ['malaria','dengue','ili']\n",
    "\n",
    "train_split = 84  # - 1 #if using price diff\n",
    "season = 0\n",
    "normalization = True\n",
    "minmaxscale = False\n",
    "\n",
    "path='/content/gdrive/MyDrive/Colab Notebooks/REN/'\n",
    "fileformat = path+'health/Monthly %s cases in india between 2006 and 2015.csv'\n",
    "\n",
    "output_folder = 'health'\n",
    "\n",
    "### All together\n",
    "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
    "activation = 'tanh'#,'relu']\n",
    "epochs=[100]\n",
    "optimizers = ['adam']#,'rmsprop']\n",
    "units = [1,2, 5,10]\n",
    "kernel_regularizers = ['l2']\n",
    "#learning_rates = [0.001,0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTRSJNJP8ah5",
    "outputId": "d27282ab-ebe1-4df3-c503-2444f89b5494"
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for disease in diseases:\n",
    "  for kernel_regularizer in kernel_regularizers:\n",
    "    for optimizer in optimizers:\n",
    "      for drpt in dropouts:\n",
    "        for hidden_units in units:\n",
    "              epochs=250\n",
    "              learning_rate=0.01\n",
    "              model_suffix = \"disease=%s_season=%d_activation=%s_optimizer=%s_learning_rate=%.4f_dropout=%.2f_units=%s_epochs=%d_train=2010-15_kreg=%s\"\\\n",
    "              %(disease,season,activation,optimizer,learning_rate,drpt,hidden_units,epochs,kernel_regularizer)\n",
    "              print(str(i),model_suffix)\n",
    "              i+=1\n",
    "\n",
    "              incdnc = pd.read_csv(fileformat%(disease),thousands=r',')['Cases'].astype(float)\n",
    "\n",
    "              ### price diff\n",
    "              #food_price = food_price.diff().dropna()\n",
    "              ### Return rate\n",
    "              #food_price = np.log(food_price).diff()[1:]\n",
    "              incdnc = np.array(incdnc).reshape(incdnc.shape[0],1)\n",
    "\n",
    "              mean = np.mean(incdnc)\n",
    "              std = np.std(incdnc)\n",
    "              maxx = incdnc.max()\n",
    "              minn = incdnc.min()\n",
    "              print (incdnc[0])\n",
    "              if normalization:\n",
    "                  incdnc = (incdnc - mean)/std\n",
    "              elif minmaxscale:\n",
    "\n",
    "                  incdnc = (incdnc - min)/(maxx - minn)\n",
    "\n",
    "              ### Events\n",
    "              events_train = np.load(os.path.join(path,'embeddings/events_train.npy'))\n",
    "              events_test = np.load(os.path.join(path,'embeddings/events_test.npy'))\n",
    "              events = np.append(events_train,events_test, axis=0)#[1:]\n",
    "\n",
    "              ### Topics\n",
    "              topics_train = np.load(os.path.join(path,'embeddings/toi.maxLDA_train.npy'))\n",
    "              topic_test = np.load(os.path.join(path,'embeddings/toi.maxLDA_test.npy'))\n",
    "              topics = np.append(topics_train,topic_test, axis=0)#[1:]\n",
    "\n",
    "              ### Word2Vec\n",
    "              w2v_train = np.load(os.path.join(path,'embeddings/toi.W2V_train.npy'))\n",
    "              w2v_test = np.load(os.path.join(path,'embeddings/toi.W2V_test.npy'))\n",
    "              w2v = np.append(w2v_train,w2v_test, axis=0)#[1:]\n",
    "\n",
    "              ### Doc2Vec\n",
    "              d2v_train = np.load(os.path.join(path,'embeddings/toi.D2V_train.npy'))\n",
    "              d2v_test = np.load(os.path.join(path,'embeddings/toi.D2V_test.npy'))\n",
    "              d2v = np.append(d2v_train,d2v_test, axis=0)#[1:]\n",
    "\n",
    "              X_event, y_event   = addSpanLag(incdnc,events,look_back=3,look_ahead=0,season=season,span=30)\n",
    "              X_event_train = X_event[:train_split]\n",
    "              y_event_train = y_event[:train_split]\n",
    "              X_event_test = X_event[train_split:]\n",
    "              y_event_test = y_event[train_split:]\n",
    "\n",
    "              X_topics, y_topics   = addSpanLag(incdnc,topics,look_back=3,look_ahead=0,season=season,span=30)\n",
    "              X_topics_train = X_topics[:train_split]\n",
    "              y_topics_train = y_topics[:train_split]\n",
    "              X_topics_test = X_topics[train_split:]\n",
    "              y_topics_test = y_topics[train_split:]\n",
    "\n",
    "              X_w2v, y_w2v   = addSpanLag(incdnc,w2v,look_back=3,look_ahead=0,season=season,span=30)\n",
    "              X_w2v_train = X_w2v[:train_split]\n",
    "              y_w2v_train = y_w2v[:train_split]\n",
    "              X_w2v_test = X_w2v[train_split:]\n",
    "              y_w2v_test = y_w2v[train_split:]\n",
    "\n",
    "              X_d2v, y_d2v   = addSpanLag(incdnc,d2v,look_back=3,look_ahead=0,season=season,span=30)\n",
    "              X_d2v_train = X_d2v[:train_split]\n",
    "              y_d2v_train = y_d2v[:train_split]\n",
    "              X_d2v_test = X_d2v[train_split:]\n",
    "              y_d2v_test = y_d2v[train_split:]\n",
    "\n",
    "\n",
    "              X_noevent, y_noevent   = addSpanLag(incdnc,look_back=3,look_ahead=0,season=season,span=30)\n",
    "              X_noevent_train = X_noevent[:train_split]\n",
    "              y_noevent_train = y_noevent[:train_split]\n",
    "              X_noevent_test = X_noevent[train_split:]\n",
    "              y_noevent_test = y_noevent[train_split:]\n",
    "\n",
    "\n",
    "\n",
    "              ### REN\n",
    "              model_event = ren_train(X_event_train,y_event_train,hidden_units=hidden_units, \\\n",
    "                                      activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs, kernel_regularizer=kernel_regularizer)\n",
    "              model_event.save(os.path.join(path,output_folder+'_models','ren_'+model_suffix))\n",
    "              print(model_event.summary())\n",
    "              ### LSTM\n",
    "              model_noevent = ren_train(X_noevent_train,y_noevent_train,hidden_units=hidden_units, \\\n",
    "                                        activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n",
    "              model_noevent.save(os.path.join(path,output_folder+'_models','lstm_'+model_suffix))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "              others=True\n",
    "\n",
    "              if others:\n",
    "                  ### LDA\n",
    "                  model_topics = ren_train(X_topics_train,y_topics_train,hidden_units=hidden_units, \\\n",
    "                                         activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n",
    "                  model_topics.save(os.path.join(path,output_folder+'_models','lda_'+model_suffix))\n",
    "                  ### W2V\n",
    "                  model_w2v = ren_train(X_w2v_train,y_w2v_train,hidden_units=hidden_units, \\\n",
    "                                         activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n",
    "                  model_w2v.save(os.path.join(path,output_folder+'_models','w2v_'+model_suffix))\n",
    "\n",
    "                  ### D2V\n",
    "                  model_d2v = ren_train(X_d2v_train,y_d2v_train,hidden_units=hidden_units, \\\n",
    "                                         activation=activation, dropout=drpt, optimizer=optimizer,epochs=epochs,kernel_regularizer=kernel_regularizer)\n",
    "                  model_d2v.save(os.path.join(path,output_folder+'_models','d2v_'+model_suffix))\n",
    "\n",
    "              e_res_tr = model_event.evaluate(X_event_train,y_event_train)\n",
    "              e_rmse_tr = np.sqrt(e_res_tr)*std\n",
    "              e_res = model_event.evaluate(X_event_test,y_event_test)\n",
    "              e_rmse = np.sqrt(e_res)*std\n",
    "\n",
    "\n",
    "              l_res_tr = model_noevent.evaluate(X_noevent_train,y_noevent_train)\n",
    "              l_rmse_tr = np.sqrt(l_res_tr)*std\n",
    "              l_res = model_noevent.evaluate(X_noevent_test,y_noevent_test)\n",
    "              l_rmse = np.sqrt(l_res)*std\n",
    "\n",
    "\n",
    "              if others:\n",
    "                t_res_tr = model_topics.evaluate(X_topics_train,y_topics_train)\n",
    "                t_rmse_tr = np.sqrt(t_res_tr)*std\n",
    "                t_res = model_topics.evaluate(X_topics_test,y_topics_test)\n",
    "                t_rmse = np.sqrt(t_res)*std\n",
    "\n",
    "                w_res_tr = model_w2v.evaluate(X_w2v_train,y_w2v_train)\n",
    "                w_rmse_tr = np.sqrt(w_res_tr)*std\n",
    "                w_res = model_w2v.evaluate(X_w2v_test,y_w2v_test)\n",
    "                w_rmse = np.sqrt(w_res)*std\n",
    "\n",
    "                d_res_tr = model_d2v.evaluate(X_d2v_train,y_d2v_train)\n",
    "                d_rmse_tr = np.sqrt(d_res_tr)*std\n",
    "                d_res = model_d2v.evaluate(X_d2v_test,y_d2v_test)\n",
    "                d_rmse = np.sqrt(d_res)*std\n",
    "\n",
    "              print(\"%s,%d,%d,%s,%s,%s,%.4f,%.2f,%d,\\\n",
    "              ren,%.3f,%.3f,\\\n",
    "              lstm,%.3f,%.3f,\\\n",
    "              lda,%.3f,%.3f,\\\n",
    "              w2v,%.3f,%.3f,\\\n",
    "              d2v,%.3f,%.3f,%s\\n\"\\\n",
    "              %(disease,season,epochs,activation,optimizer,kernel_regularizer,learning_rate,drpt,hidden_units,\n",
    "                 e_rmse_tr,e_rmse,\\\n",
    "                 l_rmse_tr,l_rmse,\\\n",
    "                t_rmse_tr,t_rmse,\\\n",
    "                w_rmse_tr,w_rmse,\\\n",
    "                d_rmse_tr,d_rmse,\\\n",
    "                kernel_regularizer))\n",
    "              \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV1z0DO7pTMv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
