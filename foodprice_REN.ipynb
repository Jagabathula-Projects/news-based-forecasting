{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28174,
     "status": "ok",
     "timestamp": 1694798194105,
     "user": {
      "displayName": "Lakshminarayanan Subramanian",
      "userId": "05209319424673529095"
     },
     "user_tz": 240
    },
    "id": "av8A69_jjKv6",
    "outputId": "d4f61290-0604-4853-c957-958e5e7d4dc6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle as cp\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import datetime as dt\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-hpirjye8Gl"
   },
   "outputs": [],
   "source": [
    "hidden_units = 200\n",
    "epochs = 100\n",
    "lookback = 7\n",
    "learning_rate = 0.001\n",
    "batch_size = 1024\n",
    "regularizer = 'l2'\n",
    "dropouts = {'dropout':0.55,'recurrent_dropout':0.3}\n",
    "\n",
    "crops = ['Onion','Potato','Wheat','Rice']\n",
    "crop = crops[0]\n",
    "\n",
    "train_split = 2557 - 1 #if using price diff\n",
    "\n",
    "fileformat = 'food_price_data/%s.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ren_train(X, y, hidden_units=100, learning_rate=0.01, epochs=200, batch_size=256,dropouts={'dropout':0.25,'recurrent_dropout':0.25},regularizer='l2'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_units, activation='relu', recurrent_regularizer=regularizer, activity_regularizer=regularizer, \n",
    "                   dropout=dropouts['dropout'], recurrent_dropout=dropouts['recurrent_dropout'], input_shape=(X.shape[1], X.shape[-1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(X, y, epochs=epochs, verbose=1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPDTimeSeries(crop,start,end):\n",
    "    startdate = dt.datetime.strptime(start, '%Y-%m-%d').date()\n",
    "    enddate = dt.datetime.strptime(end, '%Y-%m-%d').date()\n",
    "    data = pd.read_csv(fileformat%(crop))\n",
    "    print(data.head())\n",
    "    price = data['price']\n",
    "    dates = data['date']\n",
    "    \n",
    "    dateindex = pd.DatetimeIndex(dates)\n",
    "    print(dateindex[0]+31)\n",
    "    print (dateindex)\n",
    "    price_data = pd.Series(price)#, index=dates)#index)\n",
    "    return price_data[start:end]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSpanLag(data, events=[], look_back=7, look_ahead = 7):##span==1\n",
    "#     ### Adding span (required for weekly, monthly average data)\n",
    "#     if span>1:\n",
    "#         nts = []\n",
    "#         ind = 0\n",
    "#         while ind < len(ts):\n",
    "#             tmpts = ts[ind:ind+span]\n",
    "#             print (tmpts)\n",
    "#             dt = tmpts[0]\n",
    "#             vals = [k for k in tmpts]\n",
    "#             pr = sum(vals)/len(vals)\n",
    "#             nts.append((dt,pr))\n",
    "#             ind+=span\n",
    "#         ts = nts\n",
    "#     if not convert_to_Xy:\n",
    "#         return ts\n",
    "\n",
    "    ### Adding lag (based on look back and look ahead)\n",
    "    \n",
    "    if len(events) > 0:\n",
    "        input = np.append(data,events,axis=1)\n",
    "    else:\n",
    "        input = data\n",
    "    #     season = np.array([k%365 for k in range(train_split)])\n",
    "    #     season = season.reshape(len(season),1)\n",
    "    #     input = np.append(input,season,axis=1)\n",
    "    ind = 0\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    while ind + look_back + look_ahead < len(data):\n",
    "        \n",
    "        X = input[ind:ind+look_back]\n",
    "        Y = data[ind+look_back+look_ahead]\n",
    "        X_data.append(X)\n",
    "        y_data.append(Y)\n",
    "        ind+=1\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading food prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_price = pd.read_csv(fileformat%(crop))['price']\n",
    "food_price_diff = food_price.diff().dropna() \n",
    "price_diff = np.array(food_price_diff).reshape(food_price_diff.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Events\n",
    "events_train = np.load('embeddings/events_train.npy')[1:]\n",
    "events_test = np.load('embeddings/events_test.npy')\n",
    "events = np.append(events_train,events_test, axis=0)\n",
    "\n",
    "### Topics\n",
    "topics_train = np.load('embeddings/toi.LDA_train.npy')[1:]\n",
    "topic_test = np.load('embeddings/toi.LDA_test.npy')\n",
    "topics = np.append(topics_train,topic_test, axis=0)\n",
    "\n",
    "### Word2Vec\n",
    "w2v_train = np.load('embeddings/toi.W2V_train.npy')[1:]\n",
    "w2v_test = np.load('embeddings/toi.W2V_test.npy')\n",
    "w2v = np.append(w2v_train,w2v_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_event, y_event   = addSpanLag(price_diff,events,look_back=7,look_ahead=0)\n",
    "X_event_train = X_event[:train_split]\n",
    "y_event_train = y_event[:train_split]\n",
    "X_event_test = X_event[train_split:]\n",
    "y_event_test = y_event[train_split:]\n",
    "\n",
    "X_topics, y_topics   = addSpanLag(price_diff,topics,look_back=7,look_ahead=0)\n",
    "X_topics_train = X_topics[:train_split]\n",
    "y_topics_train = y_topics[:train_split]\n",
    "X_topics_test = X_topics[train_split:]\n",
    "y_topics_test = y_topics[train_split:]\n",
    "\n",
    "X_w2v, y_w2v   = addSpanLag(price_diff,w2v,look_back=7,look_ahead=0)\n",
    "X_w2v_train = X_w2v[:train_split]\n",
    "y_w2v_train = y_w2v[:train_split]\n",
    "X_w2v_test = X_w2v[train_split:]\n",
    "y_w2v_test = y_w2v[train_split:]\n",
    "\n",
    "\n",
    "X_noevent, y_noevent   = addSpanLag(price_diff,look_back=7,look_ahead=0)\n",
    "X_noevent_train = X_noevent[:train_split]\n",
    "y_noevent_train = y_noevent[:train_split]\n",
    "X_noevent_test = X_noevent[train_split:]\n",
    "y_noevent_test = y_noevent[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_event_train.shape)\n",
    "print(y_event_train.shape)\n",
    "print(X_event_test.shape)\n",
    "print(y_event_test.shape)\n",
    "\n",
    "print(X_topics_train.shape)\n",
    "print(y_topics_train.shape)\n",
    "print(X_topics_test.shape)\n",
    "print(y_topics_test.shape)\n",
    "\n",
    "print(X_w2v_train.shape)\n",
    "print(y_w2v_train.shape)\n",
    "print(X_w2v_test.shape)\n",
    "print(y_w2v_test.shape)\n",
    "\n",
    "\n",
    "print(X_noevent_train.shape)\n",
    "print(y_noevent_train.shape)\n",
    "print(X_noevent_test.shape)\n",
    "print(y_noevent_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdPtPf0i5YSH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REN\n",
    "model_event = ren_train(X_event_train,y_event_train,hidden_units=hidden_units,learning_rate=learning_rate,\n",
    "                        epochs=epochs,dropouts=dropouts,regularizer=regularizer)\n",
    "\n",
    "### LSTM + LDA\n",
    "model_topics = ren_train(X_topics_train,y_topics_train,hidden_units=hidden_units,learning_rate=learning_rate,\n",
    "                         epochs=epochs,dropouts=dropouts,regularizer=regularizer)\n",
    "\n",
    "### LSTM + W2V\n",
    "model_w2v = ren_train(X_w2v_train,y_w2v_train,hidden_units=hidden_units,learning_rate=learning_rate,\n",
    "                      epochs=epochs,dropouts=dropouts,regularizer=regularizer)\n",
    "\n",
    "### LSTM\n",
    "model_noevent = ren_train(X_noevent_train,y_noevent_train,hidden_units=hidden_units,learning_rate=learning_rate,\n",
    "                          epochs=epochs,dropouts=dropouts,regularizer=regularizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_event.save('models/ren')\n",
    "model_topics.save('models/lda')\n",
    "model_w2v.save('models/w2v')\n",
    "model_noevent.save('models/lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('REN\\n===============')\n",
    "tr_acc = model_event.evaluate(X_event_train,y_event_train)\n",
    "tst_acc_13_14 = model_event.evaluate(X_event_test[:730],y_event_test[:730])\n",
    "print(\"training accuracy: %.3f\\ntest accuracy 2013-14: %.3f\"%(np.sqrt(tr_acc),np.sqrt(tst_acc_13_14)))\n",
    "print()\n",
    "\n",
    "print('LDA\\n===============')\n",
    "tr_acc = model_topics.evaluate(X_topics_train,y_topics_train)\n",
    "tst_acc_13_14 = model_topics.evaluate(X_topics_test[:730],y_topics_test[:730])\n",
    "print(\"training accuracy: %.3f\\ntest accuracy 2013-14: %.3f\"%(np.sqrt(tr_acc),np.sqrt(tst_acc_13_14)))\n",
    "print()\n",
    "\n",
    "print('W2V\\n===============')\n",
    "tr_acc = model_w2v.evaluate(X_w2v_train,y_w2v_train)\n",
    "tst_acc_13_14 = model_w2v.evaluate(X_w2v_test[:730],y_w2v_test[:730])\n",
    "print(\"training accuracy: %.3f\\ntest accuracy 2013-14: %.3f\"%(np.sqrt(tr_acc),np.sqrt(tst_acc_13_14)))\n",
    "print()\n",
    "\n",
    "print('LSTM\\n===============')\n",
    "tr_acc = model_noevent.evaluate(X_noevent_train,y_noevent_train)\n",
    "tst_acc_13_14 = model_noevent.evaluate(X_noevent_test[:730],y_noevent_test[:730])\n",
    "yhat=model_noevent.predict(X_noevent_test)\n",
    "tst_acc = mean_squared_error(yhat,y_noevent_test,squared=False)\n",
    "print(\"training accuracy: %.3f\\ntest accuracy: %.3f\\ntest accuracy 2013-14: %.3f\"%(np.sqrt(tr_acc),tst_acc,np.sqrt(tst_acc_13_14)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP4Fh72a4Hm3abnJJgUwV9j",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
